{"cells":[{"cell_type":"markdown","metadata":{"id":"Y9Eo182f19Qc"},"source":["# Image classification with Vision Transformer\n","\n","**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>\n","**Date created:** 2021/01/18<br>\n","**Last modified:** 2021/01/18<br>\n","**Description:** Implementing the Vision Transformer (ViT) model for image classification."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXfeNvJe4kjv","executionInfo":{"status":"ok","timestamp":1670282428679,"user_tz":300,"elapsed":670693,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"489fbd01-7b21-4177-9af1-3cb944d4b944"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"soHPI4yN19Qd"},"source":["## Introduction\n","\n","This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n","model by Alexey Dosovitskiy et al. for image classification,\n","and demonstrates it on the CIFAR-100 dataset.\n","The ViT model applies the Transformer architecture with self-attention to sequences of\n","image patches, without using convolution layers.\n","\n","This example requires TensorFlow 2.4 or higher, as well as\n","[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n","which can be installed using the following command:\n","\n","```python\n","pip install -U tensorflow-addons\n","```"]},{"cell_type":"code","source":["pip install -U tensorflow-addons"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxziR4Bk474h","executionInfo":{"status":"ok","timestamp":1670282451694,"user_tz":300,"elapsed":6141,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"768d00e9-28ee-45c1-f5ba-0013b3be3229"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.18.0\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"L-fcI0pY50GS"}},{"cell_type":"markdown","metadata":{"id":"TqsV5NLG19Qe"},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb5cccad19Qf"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","import pandas as pd\n","from keras.preprocessing.image import ImageDataGenerator\n","import glob\n","import cv2"]},{"cell_type":"code","source":["# df is the training data csv\n","df = pd.read_csv(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_training_data.csv')\n","labels = pd.read_csv(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_training_labels.csv')\n","df['label'] = labels\n","\n","df['binary_label'] = np.where(df['label']== 'CE', 0, 1)  #CE is 0, LAA is 1\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Wjyzu7xD5YK0","executionInfo":{"status":"ok","timestamp":1670283568266,"user_tz":300,"elapsed":712,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"11125b35-52d1-4dbb-d097-18656087f03f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                path       image_id  width  \\\n","0  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...   b6fffe_0.jpg    400   \n","1  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...     set114.jpg    400   \n","2  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...        111.jpg    400   \n","3  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...  CEset3305.jpg    400   \n","4  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...   5c92f8_0.jpg    400   \n","\n","   height  KB_size label  binary_label  \n","0     400   47.359    CE             0  \n","1     400   19.391   LAA             1  \n","2     400   28.133   LAA             1  \n","3     400   25.210    CE             0  \n","4     400   26.142    CE             0  "],"text/html":["\n","  <div id=\"df-c4fdb47c-a060-4dba-9f2b-199b210f921c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>KB_size</th>\n","      <th>label</th>\n","      <th>binary_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>b6fffe_0.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>47.359</td>\n","      <td>CE</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>set114.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>19.391</td>\n","      <td>LAA</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>111.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>28.133</td>\n","      <td>LAA</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>CEset3305.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>25.210</td>\n","      <td>CE</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>5c92f8_0.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>26.142</td>\n","      <td>CE</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4fdb47c-a060-4dba-9f2b-199b210f921c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c4fdb47c-a060-4dba-9f2b-199b210f921c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c4fdb47c-a060-4dba-9f2b-199b210f921c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"3oANCrLh19Qf"},"source":["## Prepare the data"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"oJYuraLJ19Qf","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1670287392449,"user_tz":300,"elapsed":127,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"726db33d-8c64-401f-c899-01f6f2df2c50"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                path       image_id  width  \\\n","0  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...     set749.jpg    400   \n","1  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...        126.jpg    400   \n","2  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...     set419.jpg    400   \n","3  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...  CEset3209.jpg    400   \n","4  C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...  CEset7219.jpg    400   \n","\n","   height  KB_size label  binary_label  \n","0     400   31.156   LAA             1  \n","1     400   19.723   LAA             1  \n","2     400    8.236   LAA             1  \n","3     400   12.101    CE             0  \n","4     400   22.926    CE             0  "],"text/html":["\n","  <div id=\"df-9a9c3067-cb14-4023-962c-4d4590210f70\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>image_id</th>\n","      <th>width</th>\n","      <th>height</th>\n","      <th>KB_size</th>\n","      <th>label</th>\n","      <th>binary_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>set749.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>31.156</td>\n","      <td>LAA</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>126.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>19.723</td>\n","      <td>LAA</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>set419.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>8.236</td>\n","      <td>LAA</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>CEset3209.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>12.101</td>\n","      <td>CE</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>C:\\Users\\Lenovo\\Desktop\\ML_coursework\\project\\...</td>\n","      <td>CEset7219.jpg</td>\n","      <td>400</td>\n","      <td>400</td>\n","      <td>22.926</td>\n","      <td>CE</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a9c3067-cb14-4023-962c-4d4590210f70')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9a9c3067-cb14-4023-962c-4d4590210f70 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9a9c3067-cb14-4023-962c-4d4590210f70');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}],"source":["num_classes = 2\n","input_shape = (32, 32, 3)\n","\n","#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n","\n","# Assign testing data csv and testing label\n","testdf = pd.read_csv(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_testing_data.csv')\n","testlabel = pd.read_csv(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_testing_labels.csv')\n","\n","testdf['label'] = testlabel\n","testdf['binary_label'] = np.where(testdf['label']== 'CE', 0, 1)\n","testdf.head()"]},{"cell_type":"code","source":["# Since this transformer is designed for 32x32, we are resizing the images and creating train_datagen, test_datagen\n","train_datagen = ImageDataGenerator(zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15)\n","test_datagen = ImageDataGenerator()\n","\n","train_generator = train_datagen.flow_from_dataframe(dataframe=df, directory ='/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_train',\n","                                                    x_col='image_id', y_col='label',\n","                                                    target_size=(32, 32),batch_size=32,shuffle=True,class_mode='binary')\n","\n","test_generator = test_datagen.flow_from_dataframe(dataframe = testdf, directory= '/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_test',\n","                                                  x_col = 'image_id', y_col = 'label',\n","                                                  target_size=(32,32),batch_size=32,shuffle=False,class_mode='binary')\n","\n","x_train =np.array([cv2.resize(cv2.imread(file),(32,32),interpolation = cv2.INTER_AREA) for file in glob.glob(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_train/*.jpg')])\n","x_test = np.array([cv2.resize(cv2.imread(file),(32,32),interpolation = cv2.INTER_AREA) for file in glob.glob(r'/content/drive/MyDrive/Machine Learning project/Balanced_training_set/Balanced/balanced_test_2/*.jpg')])\n","\n","y_train = labels\n","y_test = testlabel\n","\n","# # (x_train, y_train), (x_test, y_test) = (df,labels),(testdf,testlabel)\n","print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJL1pdNXonab","executionInfo":{"status":"ok","timestamp":1670288503374,"user_tz":300,"elapsed":24054,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"2e313dbe-2612-412a-d434-12bf68425a7d"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2085 validated image filenames belonging to 2 classes.\n","Found 522 validated image filenames belonging to 2 classes.\n","x_train shape: (2085, 32, 32, 3) - y_train shape: (2085, 1)\n","x_test shape: (522, 32, 32, 3) - y_test shape: (522, 1)\n"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","lb = LabelEncoder()\n","y_train = lb.fit_transform(y_train)\n","y_test = lb.fit_transform(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7INJQqTEN9v","executionInfo":{"status":"ok","timestamp":1670288576318,"user_tz":300,"elapsed":460,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"a3128697-2182-46f4-ebf8-33c2a3de7682"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]}]},{"cell_type":"code","source":["\n","\n","\n","# test_images = glob.glob(r'/content/drive/MyDrive/Machine Learning project/train/test/*')\n","\n"],"metadata":{"id":"T9OfYfJ39dJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n","print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v8GVgROC_7Ws","executionInfo":{"status":"ok","timestamp":1670288577330,"user_tz":300,"elapsed":97,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"9b55efb8-3106-49b3-b62e-957e2672b10d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape: (2085, 32, 32, 3) - y_train shape: (2085,)\n","x_test shape: (522, 32, 32, 3) - y_test shape: (522,)\n"]}]},{"cell_type":"code","source":["# from PIL import Image\n","# import gc\n","# class VisualizeData:\n","    \n","#     @staticmethod\n","#     def visualize_train_data(train_dataframe,label='any'):\n","#         if label != 'any':\n","#             train_dataframe = train_dataframe[train_dataframe['label']==label]\n","#         data = train_dataframe.sample(6)\n","#         plt.figure(figsize=(15,15))\n","#         for i in range(6):\n","#             plt.subplot(2,3,i+1)\n","#             img = Image.open(data.iloc[i]['train_image_paths'])\n","#             fac = int(max(img.size)/224)\n","#             h, w = img.size\n","#             plt.imshow(img.resize((int(h/fac), int(w/fac))))\n","#             plt.title(f'label: {data.iloc[i].label}, image_size: {img.size}')\n","#             plt.xlabel(f'image_id : {data.iloc[i].image_id}')\n","#             del img, fac\n","#             gc.collect()\n","#         plt.tight_layout()\n","#         plt.show()\n","        \n","#     @staticmethod\n","#     def visualize_test_data(test_dataframe):\n","#         data = test_dataframe.sample(4)\n","#         plt.figure(figsize=(15,15))\n","#         for i in range(4):\n","#             plt.subplot(2,3,i+1)\n","#             img = Image.open(data.iloc[i]['test_image_paths'])\n","#             fac = int(max(img.size)/224)\n","#             h, w = img.size\n","#             plt.imshow(img.resize((int(h/fac), int(w/fac))))\n","#             plt.title(f'image_size : {img.size}')\n","#             plt.xlabel(f'image_id : {data.iloc[i].image_id}')\n","#             del img, fac\n","#             gc.collect()\n","#         plt.tight_layout()\n","#         plt.show()\n"],"metadata":{"id":"37IYX8nI-eF2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VD = VisualizeData()"],"metadata":{"id":"SUYxPhr4-zTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VD.visualize_train_data(train_df_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":171},"id":"BM-wikf2-4it","executionInfo":{"status":"error","timestamp":1668728734848,"user_tz":300,"elapsed":160,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"7e53aa96-9eec-48b3-db95-70b88d923f7c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-4c8068b852f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_df_data' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"yQIponnb19Qg"},"source":["## Configure the hyperparameters"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"PUhHlMYh19Qg","executionInfo":{"status":"ok","timestamp":1670288625288,"user_tz":300,"elapsed":134,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 100 #256\n","num_epochs = 10\n","image_size = 72  # We'll resize input images to this size\n","patch_size = 6  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n"]},{"cell_type":"markdown","metadata":{"id":"SZgt1VLZ19Qg"},"source":["## Use data augmentation"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"6EgT6xV419Qg","executionInfo":{"status":"ok","timestamp":1670288638798,"user_tz":300,"elapsed":1636,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(image_size, image_size),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","# Compute the mean and the variance of the training data for normalization.\n","data_augmentation.layers[0].adapt(x_train)\n","# train_datagen = ImageDataGenerator(zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15)\n","# test_datagen = ImageDataGenerator()\n","\n","# train_generator = train_datagen.flow_from_dataframe(dataframe=df, directory ='/content/drive/MyDrive/Machine Learning project/train/train',\n","#                                                     x_col='new_image_name', y_col='label',\n","#                                                     target_size=(224, 224),batch_size=32,shuffle=True,class_mode='binary')\n","\n","# test_generator = test_datagen.flow_from_dataframe(dataframe = testdf, directory= '/content/drive/MyDrive/Machine Learning project/train/test',\n","#                                                   x_col = 'new_image_name', y_col = 'label',\n","#                                                   target_size=(224,224),batch_size=32,shuffle=False,class_mode='binary')"]},{"cell_type":"markdown","metadata":{"id":"CYlj9lql19Qg"},"source":["## Implement multilayer perceptron (MLP)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"aKMk9Ulg19Qh","executionInfo":{"status":"ok","timestamp":1670288641371,"user_tz":300,"elapsed":103,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["\n","def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n"]},{"cell_type":"markdown","metadata":{"id":"TjeGLNjX19Qh"},"source":["## Implement patch creation as a layer"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"IDRK-75S19Qh","executionInfo":{"status":"ok","timestamp":1670288643415,"user_tz":300,"elapsed":160,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["\n","class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n"]},{"cell_type":"markdown","metadata":{"id":"Y0gHmD_S19Qh"},"source":["Let's display patches for a sample image"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"FPmTrorB19Qh","colab":{"base_uri":"https://localhost:8080/","height":548},"executionInfo":{"status":"ok","timestamp":1670288652479,"user_tz":300,"elapsed":6668,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"96165ed2-89f2-4af6-9f0c-4099f9603200"},"outputs":[{"output_type":"stream","name":"stdout","text":["Image size: 72 X 72\n","Patch size: 6 X 6\n","Patches per image: 144\n","Elements per patch: 108\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 288x288 with 144 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAO0AAADnCAYAAADy1tHpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19S48s2XbWt/aOyKw6VafPffS9bmx8ZWEwMuZlIVsCISGwMFMGMGWEmDBhBlOEZH4AQmKAGCKEhACZORMQQkiYy7UlQFh+Yndfd/e9XacemRmx92Kw1n7EMyOrsh5ZZ3/deTIrM2JnROResdZej28RM6OgoOB0YJ77AAoKCg5DEdqCghNDEdqCghNDEdqCghNDEdqCghNDNffh7e3NtG95gdP54uKCwuubm5vOHod6rS8vLyfH6h7X/nEv8rGur7s7EA22nx1r5hwPxWOM9dTHNLfBZTbW9YKxur9Ed/OLi3w+XD/wHNNY1/35cCDyefr+AWNRb6wcs0L7JNEg5iXy39tl/x6Hid7j4CnDaVPftehaLbxRHXw+B94AHwOvMaI5K7RPiv7Vnbna7H18HSfcC5ggBQVPgecV2iCYzOkuzrz39siulRdEAJn4mkxZohe8fjy7ps2FlfVZhJYnF0hR05IBjAdAICIwczKLi+YteKV4VqHloGGZ4Z0TwfVehZIntW672QAAjLUgW4mJXFUga5/4DAoKnh7Pbx5HQXVgz/KsAjwltH63kxdVBQMS09ha2bZo2IJXjicT2jHPIyNp26Bh2bmodcEe8CNCm61pyXrAmNfpJixYjNP79ff4bWY+ezZNy8yA94BzYO/hdw3YO/i2hW+b5JwaEUa32cqLFcMYAxgLripZzzKLEBcUvFI8rdCOaVsVXvaiYTk8wvtj+3gXn9l7MYmDgB8aAirm9OvDC1G7czPrIYf4fGtaZngnWtU3DdrbG/imEU2724HBIGPnwzhE+kB6FBS8cjyr0HLbwu+2cNstdldXcLutCO52CxBg12cwq9Vw3yisQWAzaS2CW/DK8XRCm5u5GofNnU+sGtfvdvCNeIfJjodxRtPuirB+2HghJvFT4Ok1bUycUE273aK9u8Pu6grt3Z1o3s0GIMIqxGv7UKGloG3RfxQUvF48myOKmcGuVe26RXt7i/b2Bm67QXt3BzIGdrWCqeuhZqVcYFHktOCDwguKjfCop3gMS6tSCgpeI6iwMRYUnBZekKYtKChYgiK0BQUnhllH1By9SOSb6IRyui8u334UF59XP/whx8+Y4dsWmy+/QHN1hd3VFd7/1m+gef9evMmbO5C1ePPJH8L6m98EAPz0X/8bcazP/u//YQAwVQW7XoNInFZU1dFBNbfufekUMccc60Ohm5lc5ekHkuPu9ZFXkwHf+NaPxLF+8OXnTADIGBCZjsNzfE5131tKN7NkWfrR27eH083cD3sOhlmqeZjBzsO3Dty24F0Dv9M47W4HslaqfTwPvMMhS0qe87BPQcEYOD6zpsZ652I6bGdL78EQE5QsZZmxL2d+HVloZwQ23PG8XjjnwW1KqnDbHfx2A7fdwm22MFUlwuz94A6XCy0Z1aov55oWvEhkZaDs4V2TqsXyrfQ9RgU2RgSWX9b82kPstl+FJ4t4ni4mmCJ54btU8oR9PNjn7/NktV2XF4p6zwUFCfkUkvnUnWOD7aM57RObCjEI1Ns+mMyPdOAzeKCm1ZPwPtbAdsyOd1+LW7a7bXdP18KzA7OD14sTH7omJWMkjbGvaSs5bGOtaltTBLZgHAQk94vOT/ZwWgLa5wJ1mkLLvhL6IiItXLEIa10Qicg+05x7gNBm3E7eiynLLFU7brhW8G2T/iCSUjxdXzA8OP6HVAxgaLQ2loxVTjcV2tzxVIS3YASqP6MvRcgWhuZxfE+1KhGBLGBYlImBCK5YzCcmtB2zQzUts4drmq6AKlwj7wXNyD4zP0BgUgG14WEB1aQDTWsyBkYqZvGHgbnfmACaYgHMXsZlF8Ozh8+oeAM6CsfJ/DIAPDOIjMxT73Ve3utEHoyHmcd6N/LOod1t5fn2Fu12i75Tqrm9BQgwtoKpaxF09dQxEVBZoK50/cCgugatVqOleaau5UWnRK8IbsEUUtjHexcZUoIiydHuhDSQjIUxVoS2qnUpZlEBIGthQJG996nxYO9xiH8J60QL1zZwvfUrALhM+5I14kUOF5MANgZsCeCgZQ3IGkDN3xydwnhK3uMitofh0BTWue0fIx+cJ/4aftM03W53/1zTjtMZeefkXDho16QQDAjMHsQG6UZw6FlNncNyHMERlda0rm3hdju47XawwPe7XRQsMjathaFxWGsAW6Vxq1poUSuL/in2hRbQ9UWR2oIR8NgfU6T4rDcn43XbRGXErL6XGPXAs8y5h5vHLLSnLtDGbO6wu70ZbNpuN7qPFLfHsA4y83hVyTrCkGQ31bV4iuc0bUHBHFL5dpLR2XSCEBGRf5gI5J3MOaZUD65q6Tn0xHGSKzTWGpkoRhb4MU7rvcRk9Y4Wr18wcw2BjZrFRv7ue+lKad7LxUv+ZSSaTzGkOOoH6eUAEFJkgh6BZOE+1vWDQj7J0lDKGC8pib5pBocTPMq+svBtG/cDPGAAqi3IVYA3MJWFqSpQXY3GaQsKDgNpiFDmFnmA69VoyKZenckeJgm2sRWMrTTyYUFk8WxeKDzYPEZczAcNGxxSg03Vlc6dnE8fGWLIGlBlQWzAzHKRKnFIFRQ8BIFhl8jAGAMGYKt6VM1ZjUzk8X8yVQxVGn1+Tmvv2RtwTSK7JsUcLjgqZqp25kkDX0ZhSmGuKCg4MRTbs6DgxFCEtqDgxLCHueKac/O5Y0rHih6P7Vc/xO0Xn8Pttrj9/A+wvfoKzIyf/1t/Oy4Avvvv/jUTEaqzM9QXlwARnGvAvkXb7LB9/xXaZpeyT6oKlx9/gvN3XweB8BN/5uc6TAX7T60wV+wbZ8nSKGdiuJ5hYljid8jPb26svSDgMmOuuL657ufyRKRSO449owJzBVjCkF//9ieJueIPPguVAp3cdorlnz0nVO+8c7aJq/fvO0c1doXmLsKDmStSx3aNsXoPr0Xq0o9np48t/G4zSDbxzQ4EwFsr4R8isG/B7FKjrbCTppHFjJSCgmMgCCAg+QB+xNDUUE7IGYivX1C99kKhzfI2ndc7lodvdmDvJXVxp6wT2zu4ze1gBB8SsSsD29Ra6SOVQdy2gPOgQC1DmhUVvvplXKuCUwdBK3UY5CH1dv1NsgqyYd3sy/AeHx7yycqbEkGWi6YHtFXlYDfn5HyV7gN+yGIhmjZL/o/ZG89/oQpeCQgg1qyosY/zjKieGZz+Ot58vM9Ii4Q2pByGJArvWrBzcLtdfG43G7jtBm6zgd/cDWz1oGl9ZeGblWhaTbxm10qDac+A0bugv/9JFRSMg6KyJLaDohYAgLGdzXsvXgSWa9qoDFPmk/SXbbP1rPTl8SOleaw0Hr6pwW0Dzi4OezG5wayCK99Hz1hJUfBaQfGJQEOxfSHr1jkcILRZumKWZ8xtG73IcY06UvYkqYsUBZ44k8bQHS9w8hBF2soTuIYvAlNeyFeZOtM/qSWxhGwiDenZutvtc6rnc/I5ru8yoc3Wr76RmlnfNtKasm3FJN5thbO4bUfXtK4RjzHttjDbjdJ1GPUiq2kMuSDGEIwxMIZQUhgLCro4WNOC1XscyLFUSNn5JHwj9JSBvzgUzMMqWxZRpE5NS4i09pA/i+A+CmLJ2dLNT/B3GMsvntue4z/Z390du0yqD7sm97mmh2taLb3zjTBUuNC9vW2UXNypwA7NYwZAbQPXNCDnQErcxuFmwKppSTRth2WxoOCp0bN9U7eB58Uy7zGCUCmtTBDYzUZitJuNCG4j3uSQadIZQ2topZvARmsbK6GeiTEecasbLYEy/eyTgoKnQl/jhrf5+S2OZR0GAjNF4MkJDBUak2UfHFHJLB6Yx1lGVdCq7L1moDCUNmvRQc+n34UslkVDPeB7Dtyexmyr4333zEDjx5F9tvSbnprY7RAczyHE3cHmvFbPhEVCK4Lq4hrW73YSmw29d7ZbJSlvo/COmceAkEFT24imBUMK4U3UuKREbwTk9OWjxyU4npAWFCTw8E7wQsKPe8zjTLtmVKleWyqI8G6F1K0VxgoO4ZvBUGoyewf2LcAET1pmZLwYxsYkh1RUTDy4i/fv+MFNXwS34Dh42YGyg5IrZv8+BNkdi8FHa69QBPd0MGdOnxIxw9x0e6wlQ2GuKCg4MZQi+IKCE0MR2oKCE8PsmvbqB18wM8M3O7TbLXzT4O6LL7C7+grt5g6bLz5Hu7mD323Rbm7BzqG9u40FA7/wj/9JNOr/4z/8e1LSXhmYtSZOVCtNsLAw1RpkLOzqHNX5JUxV4+wb30L99h2ICD/xsz8fx/riyy9kLCKYmKvcTcaYW08sZYg4JrPDkpDP4rHC8c189lbHGowzEvLpDtp97zJjTzgmc8Wxr/tDFnlvs7Gurq4SW0vf4RmrhLIivd55v82u1/sFv+HcVcvPMcc9OsFPBbH2Ycxffr+YJUcPc/E6PSnmYr5xk+PFoXnmr+4n/c9e97yYFdrQCSCGePTZaSleePZtI/FZ9iHjf2JEjct6BhuS7mNMWuDjATYxMitbarLGSMiHARgdB+AivwUfDOaFVvt3uqaB20mzaLfbwu02aJVept1uJT6rTaVDu/sBolBJzJc8gZlAWodAkMr3vCN8LL7vH1foFWSEheC131kLngmhNUH8+/kOJce8eaytKFOqokupi87FBr3Sk0dPjmi0q12nrxHSmoGZpba2k4AyXvsY38vTK7P3njuV7hSQX6GXGuxLcrLwCI9sHVMurC9wTs0KbWhZKQUCG/imQbO5Rbu5VXqZO7jdpkMzSaYGRru3h69qAHYAWHiiSNIWY4Np7WrGRImQsY8os8MSwIKCo6CvZfP3nxmzQht6yga2RanQ2YqJ3GyFLrXdqfe3FsGNlTtdUGikxSR5yMxgNtLBjIxmRukSl0jeU8GdvExFXgseEwPz+PkFFtgjtK5tRCFGJ1QrvFCuBbsW3ouJbIyJrnAR4Ap9USNrECr80HE1peoe+ZdSEbz+PcigXOJ5nNjmNZnQj3Imc9fnyVIPQ6HKEYdciDg/5szjpXPoYA/5MswKbXMr/MWh4N23DZrNHZrtnRbAywNkYFcEWAu7XsOu1uhPKVvXADN8Q3Btq15h/dAIM14ws0XLWjCZwThAXn00LAUsa9uCo6A/h17QMmx5yMflWlYdUUroJuV1lGnaejBWZKggCdEIR6qo3k7rEepqWjGPx4VwNH5LNFoFdB8cut+i7ReOeawbz9w4r/Xm9pCzmqwoy+fVM1txe73HACeKU8/azt6CbAVTrwAQTC3alWwFuwqatguqKgnvuLyLdiB3MxLbjQ8rDzLammH/xSiatuBDwbzQqqaVkA9rrasBmQrGMmx9BiJJPbTrcxhboTo/R3X2ZjCWqddiHrtaBJI1CYN6ghofVRLcCfS1av66CG7Bo+IZHVQH0s0ghXbUFCbjYTR/WPKIK5Ad8R6TASitW6FJEdKeIb1G54HRCzImkGF9Gz4rGvf08aJ+vjHl0JtjTxV+XKRpfdC0ngEYkBVT19ZrkKkkyX99DlPVsGdvUJ2do2/SGo3dUlODyIINknAaAzZKOWOs0KuGG8GMph0cb9G0BY+MPhH/c+TBzwutCxlRgbQN0dlEFjDVSjRtvZZHVcOu1jCrs8FYpM4pspWsUz1F7zAHqhl9RGFVMvMxzFHQFG1bcFTkDqgRbcqAZPU90Vzbr8YWanzq+Yv6wjIrPGMf3XesgoJnwFMGhArdTEHBiaEwVxQUnBiK0BYUnBhmHVHf++V/z6mZtPTo8c1We/Z4+LYB2MOu1qjOzkHWojo7j8kVf/wXfjEuPn/lX/1TBoDm5kvsrj6Tsj5bAWRBqzcwb78NU5/Dnr9FffkNGFtjdXGJ+uwcRIQ/+jN/Ko716aefRrqZsUfqA5S2CYtkIlpMe7IEL32sJ6PTWYCHjNU/1g6ty/v3+2ldZvwgc8cVU2VDhw3nBsciHSAlcPnu619fdFxLTv6j7Bxz7OE91pBM6AIAFu8vAPJe5IC99OSxJtXRjnIP5YcqQkSQUA+U4wkx7BNem9Ha3P1QT7f2ty18yAUPQvAa5wIc3icIEcNLCfmkbCQCGQKDpUm7MQB7kLdJaI2NAsZ+2IArnbDUzEp8VgnZYmKGjeOQ1RTGgy8GCyNGVqxRBLbgKMiEVrSCzueYIPQ0WNZhQKLHkjscD9BIf1kOWnJewLj33B9ekp/yLKh0IYYhn7lv2XfxBoV+I0ejnxyxAddjh6mmvvupowPHJHY75jVbehmmtuNMu8ZWrnkeQPj8CbC8LQggZqzWxYq+VJM5aMjADzV3scPawAtzhbBWIEuPNDBkY6f4QzKissMc+2I8UgVqwWuGpvAm81ibp4e/SWh8X455DCBfhxLpxCeTHSPr2tMmp8/ECaTzT2ZG4IeS6qGQ0yzC+pA7bRHcgmMgzFkAwoWmAhsFF8j8Mi+gNA8hrzKRMunBQX1Uar4aFTRQ1JCDoWRLvQgMHwQW4ymIpN648eKAsG33/bFty3q24OHI/DEqsMwsAqyVZoTx+u4cxxLqeaHVnrJ5Ubo4jkTTkoZW0BHaqSR/qfJhBpwTE4OMB5xP64EQsrFWwjbGLBY6isdHg/cKCh4CIRCUHs0+sJFmQktG+MyeCsvWtLlqCxo2mrEUQzaxPcdYmIakh2zQtB0vXAjPqFc5VE2MCeJiZG7jnB5u+m4YNPzhX7XscIZuitl2j/ODPfyAjoCXcRTHxlS4Up98WtOO0R7NjnzI7zYzN2aFNgofkVq8BLIWxooNb2zQtOowItXEY5o2CCNyodVQL8KNwEShN7TMETV2bvFiOqf3mWApjG6sA018XvCBI3mKO3Falso3SQQYCXE+ImaF1lRSzB5MYIBgKo2pEoEqG83i8HnQlgNkxe0pzBUuAGRfFVJjxDw2M6V5/aEjesHv4J6fcpCVGtyCvQhzVVvfBA8yhzWt5ye1fuY1rXIVd8jIrY2ZSsba5DXrxVYHYxFNNEpK5jBA3fHocEGKmSpg6RnETr9/fH0chDZp4w8QB1zj5zKJ9xHUHVVmBoNl5i+Pm7njtds8eVyjZ5P7Y2YOb1ZoVxeXOlbQpICxmrIYBJgoE5Rsido/HqWgCY4qJo6aVeK8lZrelZDG2Wp/zDeuVbNQDvvYECw0CwOg+chqan8rjeDaRlJFbHXPlMmCV41sQt//vpAJRbD+wnKMcqW1LCQ5K7TVuTBQdNt+2OiAIl3bxoQJfXg/cnrGyOGEpAmwhoaCSWyUa8pkjyVB61xwJZYWKF9ds0O7vQOY1SM95K5i59QPZmBKHLdgBNFCzLTtcgmO3tfkfA0amEzsqgFg0krtY1a15E2a05v587QpPMU2cZhIpJjtMVAK/gteAwpzRUHBiaEs4goKTgxFaAsKTgyzjqjPf/e3OrZzSOZPYRmRee8dvDbVcm0bO7X/2E/+VFyMfvc//EtmZmx+8CmuP/ttsHMwqzOQrVFffIQ3n/wE6jdvUV++w+rdxzC2gq2q6PT6wz/+nTjWp58pc0V4aPYUQZqFba/fwzU7NHc32L3/CswetlpFb/dP/YW/Esf6/u/8JgNAtT7D6vyi40FesnK4vExsE4eyMfTX6jlzxfUcc8WCAwtsDMdkm3j/wLHeHjhWN/ze3fxQ5orZ48rHurri9F3iPHJNo72rWrSbLbxrMy8woVqfwdY1iAgf/+iPxbGuvvohx1xlTX3k2P+KNAqjTledm8icvh999NGoM+eA0rxs/z2OodE41oJhl2JIED2Mq4Xn/NVUUndJqkh4FR6OfTe1ufTRB/l45gsG8vdDhCfMy0OOY09yRQqRxOQDzS+WN0NsFPAx3KPZImMHIoOCbI3QqcDYWnoDxVCS6XifJzvmAWDvQD6kmXkQs4Z5Nul5t9E0swk/PfuUnVVQEEOHqfyOvdd5PZxDlO0zHEre920Lv9uBvYfb7eDbRsKcVQ0YA1NXsPUq5j5gT77AnjTGYcvK/l2KGfAgeAbYM7zzcNpOpLMdoDQzBsbWYLLSoaCqYao6o5tJQjsZt8pyi30rpopvG2l03TZobq/h2gbt3Q3azU06Ataq+3worWRi9pN3vYIPBzFvPTdpvdNHty1rzF+QPTCYW5nQtpuNPN/dwm02IGth12fyvFoD55Lrb7wf7YWVY17TTuUQI6QLxn86CdXj5rHaA2EtTEgFAZ0qoSzGGzR7fyzu3w197JcriRVOBNjJxUa48COJ3dHULpr2pDFqTvbfC9Zb/v5ACWVzqzenB0kWe45hcANwDr5p4JpGhNNYGGawrSKvmvCvzc/FPZo2fczZi5iMr6fBYDWPPZx3cG5M06qwkwXZFWAcTLWKD8oabvVpT0cGkyfnxUngHdx2A9ds4RvRtL5t0G5v0Wxu9f4wMZZ3YAwbURecODq/Z5bJtNAfw97DO7nhh+bpOVtFwOxoKoi+adDe3cE3DXZXV2hubmCqCvWbC5iqAjvtPGmtZO89RGi7a9qgTDk2G/KaAhjeZ5/s/9ELAtI1bQV47b4XHmQjfWpOPTMFWT87uFY8e+12C7fbwLeylvXtTgR5t5Xc4swT3RsIiKVXs9eq4FQwJrDh9T7BDVo1+Gby196PFL2E3UYcmmGstoXbbuGaBu3tLZr372HqGgCJ19lauLMzGB3DYz4LcJF5HK8BcZZKOXLwmcu6f25kKhWeGqZegZlhqjVMHda0VdYtL+Ucjx58+G7Poimdg28b+EYW+dw2QqjuWrBXre+dOp3GzaHc4zxyYjNX6ZFwCjeQR7NOpsYdr5qJpmW8AWvcIDcPEdagan7GUtLeWE58HF7nFKu/xHsnVWNK2i8rN2GsIG9h2U7LRBy8m6OPWObHUvbnHNha0IQjN2AP73Hw4E6UGKVrlAQtEI73vtaenasvyMsFYIatV9Iec30mj5X8HZ1RhqZvirpO9eqNc3c3aG+v4V2DdnMDdi1cswHvNmAisFsDrh7KX3BOhR+84IUiT7jvwutyTOpafbT8ggUVtCMZKRoRJ6cdXTKFqjC326Ld3Gm12C7mIQQfSSQzDNRLM/6fTmgymKbB/DZGi1tknQvav1Tbq2lDABmdOxfi3WxA14Lx9aixldyhqhqmWgPgqGVNVcdSvK73OB1HF0HTCm8POyfatdmJdm0beKea1jkhWg+OqP71iGVSwwt1r/DtA2KErw1zZ3rvq8BDAzWS4wdTduAQldcGUC4n6W4R9umOpdEEjUSwkyWYtMBJmtYYC7IeZAy8cyJwg5OkNH48YU7/BkdXML2JZC7vmSOH8R4jCSr3hVhNj6lWHqZex2MOd0xTi7DaegWjWjfWtWaZIZNHwj4Kq99u4Da38K6F323gfQt2O7BrAE9qHjuMqNqiYE8FU5GJYB7nYRrW8ExP60IZQ4knkhraFgCrpr1Vi20XhTZYiSbUfGvIhsesOKSwUIiE5Eu/sMwMx+0hYZ995DUHC208uY6mpVgoL9pyeGHt+o2+qIFqBbDQ2RhrtYP8uQptnYWBpgUWYMC18Lst/G4r5vHNe7BXs5gd2Ldg3wDWgJ14mYcj5j9owYvGhHnMmhfgNewXhCsoB6/7WLUaiZV1xfDA8vHNFgDgtndobq9EaLdbuHYnNw0nzihja13eVXFpN0BYXoa6cJM91IqNAts2IA1JjmrtDIcLbXbnCg7XdBmnnUdB+waWCoCT8ykzi8c0bH+86JYPaxbOzOTA4xMD4Zx+7KlYXjb+uOw+jkDPr11e901k2T0y2yiavCOxdu/k5/USn+9wE4d9gU5ngDRszzyOIR8X55R3rTg245pWjswbyTcIiRf9G4AuKkdWZCwkEJ34b5qjMXdgAgspVCe+vfNF85MwdNuzMc0fsJUIrMSoNCvKZsI7dzxg9eapgLpgGjnlUs4ekcpjarzJEyx4JnRNWk2RVYdQ/3faXX8FQBxSvmmC20l/99Sqhg2BnUQxMBWWdE6+T1MPfdvAb+/gdlsRML0pmKqGbRtwVcO/uQSfnWNy6RVDRnID8K4BwDCulV18KiKQOfuANMbRkwp3omy9wNmEjxesd2EpUr0QrOYuB3qZEFiWSodlDIwxNqwu804sTV3pMec4mgRDcygNODjkgmcCMzrCCrWcfNvCt7vB9iFVVTjBZO1JtlZhNfragJ209eBgmo7GVn0SzkbCh363g99tNeFCxodr9bhcvJmML+fCeQTl4uL6VXLnKbMAvIQxaai1c9x7TbsfPVNhgSt7eu99GDN7DxqgQPEoHt97Yjhdpn9UVmsv7RSEctn+98MxxztANkr6XkHBaaEwVxQUnBiK0BYUnBhm17Q3N9fiZopOH4bTsIr3Ht6F55Sn6Zo2euD+yJ/4mbii+PVf+9WBHU7KRUzGSKKF6SVnxOwXxo9mNB6/85u/wWDG7qsfYPPl9+G2W2y++D6aqx+oh26rDoUWQAMyFut3X0f95hJEhJ/9m383jvX7/+tXGADqi3dYf/TNvbWMfeR0LIdSu/RXJm/fZnQs77tjHZpIFelm5qhYFgw6e34HLq0uOxQx09dK5lcbPa4hs213/ZU4nZjxnT/75+NY3/u3/4IBoRpy2w1ABvXFR7BnF7D1GvXFO9h6Bbs+Q3V+AbIW9ZsL2PUaBMJHX/taHOt3v/vfmAHsvvoSd198Bt/u0FxfxcQdt92AXQuqVjArSQq6/OQ7OPvmtwEAP/kXfzGO9cWnv88AsPnyC9x8//fhthvcfPYpNj/8Eqaqsbq4hK1XWF2+xdm7r0ui0WoFqiqhWPrZPzf6Ax3giOou8IE8xJN650y6YUfirkYJySXHeC6RYgKazGFsJUK/WotHzpGEfrgCIDcFW69g6xUG7hTKWp8UvBCwhGSyxAPvHXyzFaHszYn27hqACG273QgrRC0lnzySB7//p87CTSGf2TtA02W9bzUlEmCQJO6MlKNCE/9TvFdDPm0DAsVYcIxyEGNJK5yFQpuEM7zM8ztjsfDMCHmz6NBtIIR4+okV/W+dOiIK7vyKYVdn8BDoO7kAABaoSURBVGc7TcS2Gu9yALcgQ7Crc9j6bERmbXjxIaUFv2hI9CPENRstv2zQbjcZE0lCjNOqpiVjYVdnsPVafl8N7QQCwPRFGHGJp9wDCmFD5wDXgl0D3+4khluJoBEYvtmB22Z4Hj4jcwsJGk0D30grGnYt2BiN0aZg6cOFdjSJaDqjZAqJjaLL/zSX+TQ7HpByna0RbVvVgHcgo4232AAsuaZSmDA83dSi8wgSOzNGrPM4orc+P+ZJErG5/R9+AA/adfpS9JRBoHqJmqm3dc5OEumDktXXUQh5McuCwx/KNANIcdWQD8BjVEYx8ahXxDDIzMpPaDqrMOD+GVHhAMKbPU3aR2zARZSlNGZEbkaSqpdMg1BFZOsa9Zs34HYFA6Bar9WUadTk8GB2ICLU5xew67PBWJIzSpqxVVTtS0FYy7pmJwUgbQu326Ddbgbbup0kXIT88jCRQuKOqWrYeg1b1ZrQY1NRSh+UKRMT8ukpyXy4EbADvNF6btGi/fFYs7O4bYHwcK1qbk2k8DyU2aNX+cS8X/l3wAmlQjm480fzJHWQzxt7gUJv2zR6Oov+LVm2M7ZCtToDVw6GCL5eyba+1Tuf3AGJhJvW1MOkbqqEr/ZQB1TBY0K0k88Ft22UTmiYESXryUQLA/KZJRaEtlb2ktRfeRKqjNMcRSwVJQqKKiy/5MHeDXPkc65j56TSzHmQrtcR+toyD6f4DO6VETUmkHKuqXphqgFXLqRdgcWMopsYyxjh2PEGxB7GGDVV0uI+aFq7Wo+yS+YWQP/7x67jMXXx7LxZ8EWnnhYzdY5y3w6mcVYMEqp4emceTGJokUCcjVm5KPSR+09GC1uC3Z6V1PVEMRO04KQKpnJvSy1kCALKvrdv5iNKF4P2/vgPKM3rOZ6ieWxGo78m9qeVtpYd4Z3AmI6VwTR3ua5BlVYM+XPhrmIGhzWH3rGDVh5rdWm1zpeMBY/QjxQ8Bziax75t4HaiYdud8lj34Bp1AnGmwYiET7uqYFYr2NUaVEkNLMx4zbcgCXSIaoSlG0icUwwHYgC+BZwRJ1XrBnf0cFw+kDK0ahorvQyp0KvTeNQZO4Yj5B6LaBG007uuCQZb5WvensCO3fGCNh87/DSWjUsAE7YNjgJATWQdx1i5WfTHyrzHw4OYON0HYKnDa5bU7rAvfNB33We7B4+lOcSxIMQ5NZWTo6m7fWKuSANk1lxouxG07ZzzM/OQ5Z7coSHGmWJQ5TBwkIXi/J7zKRaw5GMGm3z/db6n0GZsiQayICdZO06W6OUHsujH16s0M1YQWHEcJAXPEPIuoRQJQjvCxBhHOe6ELDg+KBPCwZzIknHADOi61Vir7BLVKBn+5B1Y32YIM2J47vSpYAZRt5a7D6/F+bFxOQBYKxq/rkB1BapryTGIj5UyNU5jmdB25C2csNFFuReCu8i5g1FBi3escIGXCsnUukOfja6HTZagQZ07Lqd9Rr2F2Vqi4MWB8n917g1mV4hGAKK9rJjFpA+jj5htt8cEZQBMBA/qCG56rVtp8kUgye8jeLUDmwYTpF48COtKs6rWK9i1MJPa9XqcBSPDPTRtEI5Q9kSa6BBOeHxd2DFJlnzL3Fo3c2ohez3vEZxxPGR4bUVPJ3krGrFYu39POCZ110Dv0iEJzD3BYUsafhmrIyXd9kmfs/dCUgbnDBRzcdrM/xOOy1C8iVDuJFNTfg6LhDasV2VRDhDnmlYOnJUXyvDExO+taR+CPA3S6I9hDMFQMpATunHkqeM60en9atGfJknw0pInftZrFEfWima1QdPWMbSXbvKz3w6ApFieoBoXnUf4J+Xet4NBk6ZtEYlcrR1q2tW6p2nXs9dmsaZNRewiuPB6RxM69HhHAUxwSfUH0Bvbw4Wjc/eMHMkm3kkPHO3Bx1PweEhrT4pJDwOl0HFoUlq/2u6jv/0U+gIac4yR1rPhtdGwlHdjcVqhpvEh9CPruGi2y9q2v6atj7SmPRQ0NJHnTJr9w+3fN+nLcCedsnPnx3pt5vFrxCFzKVjAk9sM3lb7eApPND9mz6kwVxQUnBZKEXxBwYmhCG1BwYlhdk17ff0+Oas1acJnvVKcsq3L/6GUKu3/ySefRMP8008/7eeLpFfRRO9mquSPb33rW/HDL7/8kse2STHkeV/DxUViY7i5uZ5cIezLEQG6zA43NzcPWmtcXFzEsa6v58ba/zWRueK6z4BxmOMtP6Zjnt/cWL5t0GxuNYVxg+buGr5tsXv/AzS3UvD+03/1b8Sx/ss//yX1D4l7yFQ1Lj/5Ds6/8SNYvbnE5Y/8OOo3l5PHdXmZGDV++3v/nQHG9uoHuP3i+/DNFrurL9HeXsHvNnDXX4KbLUKc1hiL1dc+QXXxdQDAz/2dfxDH+rVf/jcMaJ3vbiuZXdsNfNPArs9w9s2PUZ2dYf21b+D842/D1DWqs3OYegUiwtuP3o3+WItL88ajOCSfaQtMCXxPbLzvK9SFHiZVKDqYi62OCWvBK4BGA4ytwFUVi9nt6gxuJIkhVG+FzgLCcxx6Qi3j0e4j3LA5V0g9vzJpbJa9kzzkfpw2ZkRJPjQBIK08SlRLqZF6nnMwh4O8xykjk5F72WJJnb49z2Gx5zuyCqEpQewLbcErA2mpJBEM1rAMGC3Rq0Y6A5jVOQCk0EuI0ZoqI8lfjliA06/IGQisk3iQb0fpZjiUEWpxADO0VBCxeVfMiSapLV9yg7kXc0VHYDV+S+PR2b2DjZmg+wT3IeGjQ1DuB8+HkGscYvDgFHsdbGul0iuEGge9oY4ci6dOVEiznrwffE2nm1+Y6NSzEGNOdefMZyffQZo2P9a+lrtP6Ghql+7adF5oi6Z9fSCooHoRSMMAGwe7WsP7dnDvr84vAKhwuBZkK9iVkB4YWx08Ryj/hwzEXysPIsmUAiEWsQv300i7kjvhsxINmhEIxiKGYBFkDC5iQ88e3/KMKCDLMU6F7gNhXSi7+W5zwjt7TEVgXy8Cd5cBTAWwp0gb059zdh3MY6dNxK04c1QoRssulx2EPuXEDaEIVD9TbirfNIP56LdbPRWb0iirXqdIk5nFC62CBULb0a/6nAQ2dxzJJxONfzvvTUt2X4Pf3zxeKtC078ZW8NSIsiKTmFVYYnpib3O7kn7H7D28cTBWiPzSejEbdMFXc5YyiWDCZua63FBSgUBsrNUXWs09pqqGNQZMRuq+tWAAmbD2j25OIS3UtHoqvZM3ygXVEUgeN5Vn17vUe01D4S34kCCFAUwMYuFXYxLu6jGzbH35DgAiiT6RQXX+RiiG6pVosgO+W928CAUKxhgYY8FkAWPB1kodLQhgAjetEOT30Lx/DxDBrs9AIGFvqdcwptJHIuuPN4eHO6IIREnLRjaJMbNYkcd1ex/0h86f9A8a3+aBjqe59faStfjz3Dw+8PRSAogBJgJBSRaUmL4PqyEf74U0TcJFdYd1cXFEgwgpuz9p2WQeU/wsalolb+sjmMxsK22xmdWSU1Y2mGvyBYd4YMGACHGXfLFnzmJaoOeHTo4nE4uai7b9cEGZ4EK5xSzY1ujf0OxKqHFNqGslgq1XYiLfI+ST5JOi1ZeEKjiVKJXo+cBr0QW3QqFq6lpYQcHKwV3F3syRcK4zz4/kiOoOOFzL3ke4wh55YNnkhcuz7UIKXjNiCR5JMBHEoKrOaqYT6nPJdorNxIm0w0BoLH3oHEpatqNp+95kpsjEKMkVXfjdDiCA23XkN6ZYmmfVc6yN1Hvaew6zQiuWQjZInvmUCevBgpUPGTRs+MLOe8vYJgr6WH7XfslIP7MI8OR8iIR9ssYMIaOUaXTYd8p3xRGz3amr5Ln3PECWHRiOI1srd73SnRPG3EEfrmnVmdz38O7LUuq8py8NpbtYt7A9XPiiaQsSZF4MNW3gVIoZTESJmNyYkK6w/Htipl9GNpNlRynzaUqWmliPUmUBUCp6z8jbSOl/JXsrS7d8uCNq8qxGNW54Dl7lzi4jzqT+PkDPPC4CW5AjaKgB3YxMYwKAkLMes6LMnNKa/ioV2PzBUEH1gTYVWfLCyJeEuVxZUKXMkBnZXExjDE6phce2X2g71nH6Y5837j4e3325xgUF/bCjvBc8vkia6p5TJgjnAw6ue1ydkY+DwlxRUHBiKEXwBQUnhiK0BQUnhtk17c3N9bQzOxYJc4fRwmdlSB9//HE05D///PNBdXyIfRERrLYfnHJCPYRBob8EyNkm+swOY5hbjVzkzBVzYy1Yl3eZK/Yf1zQIl5cyljBg3H+ozrUaXPf055IVW84Ycj0zt8YPt/8bvs2u1fv+xMp2Cznx03ibjfX/fu1/MDNj9/6H2H75OXyzw+7qB2hv38PvtmiufwjfbKWpViwIWIFIqon+0i/9szjWf/pHf58BoP7oHdbf+Bi2XmP9jW+hfvtOeitfXAjX8WqN6vxNiuGqAyu/9jmeWNOWmGvBIcgLz18Ago8reHo7fqa5ebzHOXagCByN93gp20QnVF3COgUvDUSaGmwAawFvNUxTg5hhVmt4IqHk1/sJUQXQiCgFS1Kpc8hWqllX2qa1jllRMMtitMCRhDYXvLEYbXh/ar8ivAUvBkZjrkbTDL0HVStQvYYhgmnOQMbCmwogq+a3AY8ZrSH+akM3emm0VZ2dgaoKdpVqflPa7v5DPGqHgTnBWxKDLYJbIDi+OZzPrLnRY8JQTC/UdEOthyVrAdZqIms1Q8qMNrToWJ/GAKHplrWaRGSisGLEEp3C0c3j/uuAMU07tn8R3IJnBZloHpO1YhLXK3jXwhgCfAtva5BtQMZKdpRjKRwYGQsEbb0pmrYKmtZa2HoVC/WXkroBj9DL5yFZTUVgC54dQXlEofWxURYTgetWC+JNJHzg1sG3w9K8WBxgDExl4zh2tVJKHKHCiUXwj5p73DmuZYL2IQrkC/F5PglO6Vxn/bwxn75vrlI0k8HipCJbaeUAwYzU05qqjpo2luANUh0PP/7H6ZpXUPAqEEI1ykts1JNsSDQtEeAZbL3W1Xb3rs7fAADs+gymqqVIYJT6Jr9J7MdeupnJTw68Qzynpr2PgwzANE3kE2DuuF5/ung496UnOvMbHjrtBl+ZfC1MgbsKgGFZ0xKDYoP1Lkyl1Uc2MS+CqPMVh4mroGjagoIxxLwO7rJXBCYMza4gAELMPCK0q7WUndYr1dBSX5vqchnkGRyt5mUiXIS2oKCPQP7EwgaZitMtyHqARWuyqSS5osKoURDN49UaVFVAVYmm9UKfw55FY3MoCMyfp7FHaJeYJx+eg+kp8LCSydfymzzneYgWDSXkRMroqCmMHExiZZ4bK2EPLUw61DcBD/h5i6YtKOiBKLXn6BSwkAHDAlANbEInPAvADMS2OpPOB3Z9JjQzSqCezOv7oQhtQcEoSLVr3gSM0/o1Lx6gca+wWa10TVsnehmjAtt/HIAitHM43EX+OMdxIPLDGKNnufe4vb/5AePOMSLxWE7g/GD3/XC4NS3g7V6YQDSkmznS71DoZgoKTguFuaKg4MRQhLag4MRwb7qZhGk7/SH0Kcekm5k7rscc69CBL49FN0MUx7q+ueGHpFDllCfHPL9ZKqNFYy2krjlwrM9+/X8zM6O5vcbu/VfwbQu328G3jZKhZznGmiFlK+kbBAB/7C//tTjWb/3X/8yAJFnEyp7zc0m60N5EsVgg9vNJGVhTdDPFEVVQ0EcUnlRPm+KzFAnKo8Bp8UDfP2W084EN3uO8mXRW0XNoBKgIbUHBCEJclsinInUibY7HUtROEsIxVQ27qtEXvWqt3fxCKxBjpFteJ/aLwX778KRCO2XDvIxAScGheJW/W6dbAZJQZe+HTvFxIwrtSnpD5RlRJtPYkRWjuwxcej2Lpi0o6EMregAR0Lw5Hmf5wZKaaMQ0roea1p6JpiVjpeKHKPWlRRL0fC27BEVoCwqm0OFt0td5gXwwoUNBfA9GO9RHFgwi7ZebnE7d5JdlgvsihDa28HzWoyg4FK8xL4c7ZnD2fka/zJxywUhN3KA9c5ggyCbzDsf2rT0G0gOy6V6E0BYUvDR0TeLsdYc/XYXPVLDVmHm81lc9jXoA8+IYitA+Ih5iORzK9PGalN7YmT/Z+XHWxLX/pTz2ZvI092FGTOYhDp8lRWgLCnJkBfDsvXBAOQ92HvAecNKrShStEUeV90+6VihCW1CQQ6lg4FRAnQecA1qndKlttuY1EvrxHuz9k/GgPanQFkdTwUtHp+otLGK9vu8TtxMH4Q4P4Mm0bdG0BQU5WKhQOTOLOWhar681/1hiuGoee/9k9dRFaAsKcnDQqmkNGx9eTWVmgDxgPOCdvP+Ea9pSmldQcGIozBUFBSeGomkLCk4MRWgLCk4Me5grToMhoow1PdajMnMcOPLl5dLzG37Uf+exmCt+71f/JwPA7uoKmy+/EOaKzQa+EeYK0pMOLStNXePsmx+jfvcORIQf/ZN/etlxLTjiwlxRULAEoc+Od2DXfQwQ62LHOww8ForQFhTk8JKmCOfATQtuG6Bt5aFVOiKknNgnnhhFaAueCU8UtTjwa0I0JZK45dlPABBYKzIGiqcW3iK0BQU5vNbdOQ9uW8k11uQJIoKx0t7D1ivY9RpU1TB16NHzNIdYhLagIEfILdaURXbdjCeyQh0TyNpMVWmfnqcLxBShLbg3XkjroqMimsJauRPXuEBGlxoEtwJpJzyhkXmaYyxCW1CQIXiJvXPgtoVv2/gZGQO7ErO4On+D+uISVFWw6zPYui5C+6Hj8OzStENXA5bG4IdBGzKwdGqPfFDqdDLWyiNoWlvBWAsY8zrraQsKTgtqKhsDGKU/rWuY1Uof60ifaqqnE6UitAUFA7D+pwhsipWFXa1h1mewZ2ewZ+fiSV6tQEVoCwqeG4k5MbXwMGIGm/Qc+/mUOG1BwTPBWhAzqLIwdQ02BlTJutUGs7gOD/Ecp64BT4MitAUFOUKWk7GgSsI4Eoe1kkgRYrQxPpvaVT4VSmleQUGGaOZOymCfcHxk30dGYa4oKDgxFE1bUHBiKEJbUHBiuDdzxRKrOmcquL6eHmvJUuClM0Qce6zr6/uzMRABF8rGcHNzveCnmv4BLvNjmjm/Jau55dfqiMwVBzJE/N73vsvMjN1XX2Hzxedg76SLu7Wozs9x/vG3UZ2fSxrj20vJOZ4Y6yG/Iah7jjmKpi0oGENoSasaJTZ+fgEZnyXkU1CQwVQ1GCxFAW8uwN5LeMda2PUZjFb1YKQf7VOhCG1BQQazXknK8bmDb6WbQEigMHUNs16BQtH7M6EIbUFBBjI2CWpdA97HelmyVezo/hzcUAFFaAsKMtjVCgygIgKRAYNVUFPBgBTCP20WVI4itAUFGaiq1f8klT0MJAI3Y6QEzzxtrnEfL0pol2ZnLdnuOc2XkwEPXsxHSOau++jlfsLf4EiJfSmN0UTBzIVWKnqAJefGDzioOR7lFyW0BQXPjehgUorj+EcQXGsTSfkzoQjtC8XDpgRNvO7jME3Q3/opp+3cdxGOx6JMREIxQwYwietYBFVeEz1vvLYIbUFBDvUMM3sAFgCrVhWzmIy+fkapLUJbUJAjZkBZgAJ1anjKzOJiHhcUvAwQkZja2lEgfdDd5jlRhLagIEeuSbmraUf+eBYUoS0omMILDRveW2gPPZ8Xev4vF0e8YFMjFc6SEdBzupiWodDNFBScGEo9bUHBiaEIbUHBiaEIbUHBiaEIbUHBiaEIbUHBiaEIbUHBieH/AzUQVOEBJ784AAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(4, 4))\n","image = x_train[np.random.choice(range(x_train.shape[0]))]\n","plt.imshow(image.astype(\"uint8\"))\n","plt.axis(\"off\")\n","\n","resized_image = tf.image.resize(\n","    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",")\n","patches = Patches(patch_size)(resized_image)\n","print(f\"Image size: {image_size} X {image_size}\")\n","print(f\"Patch size: {patch_size} X {patch_size}\")\n","print(f\"Patches per image: {patches.shape[1]}\")\n","print(f\"Elements per patch: {patches.shape[-1]}\")\n","\n","n = int(np.sqrt(patches.shape[1]))\n","plt.figure(figsize=(4, 4))\n","for i, patch in enumerate(patches[0]):\n","    ax = plt.subplot(n, n, i + 1)\n","    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n","    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"gIjG-GRv19Qi"},"source":["## Implement the patch encoding layer\n","\n","The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n","vector of size `projection_dim`. In addition, it adds a learnable position\n","embedding to the projected vector."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"EdoILAw519Qi","executionInfo":{"status":"ok","timestamp":1670288653692,"user_tz":300,"elapsed":147,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["\n","class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded\n"]},{"cell_type":"markdown","metadata":{"id":"QI7qeR9Y19Qi"},"source":["## Build the ViT model\n","\n","The ViT model consists of multiple Transformer blocks,\n","which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n","applied to the sequence of patches. The Transformer blocks produce a\n","`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n","classifier head with softmax to produce the final class probabilities output.\n","\n","Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n","which prepends a learnable embedding to the sequence of encoded patches to serve\n","as the image representation, all the outputs of the final Transformer block are\n","reshaped with `layers.Flatten()` and used as the image\n","representation input to the classifier head.\n","Note that the `layers.GlobalAveragePooling1D` layer\n","could also be used instead to aggregate the outputs of the Transformer block,\n","especially when the number of patches and the projection dimensions are large."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"ibQeIEYo19Qj","executionInfo":{"status":"ok","timestamp":1670288661976,"user_tz":300,"elapsed":104,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}}},"outputs":[],"source":["\n","def create_vit_classifier():\n","    inputs = layers.Input(shape=input_shape)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(num_classes)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"VTjSU8nL19Qj"},"source":["## Compile, train, and evaluate the mode"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"JzFUUeka19Qj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670290161244,"user_tz":300,"elapsed":257068,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"c02d132f-d71b-48a5-8b90-eef34e6e771b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","19/19 [==============================] - 163s 8s/step - loss: 7.5264 - accuracy: 0.5144 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 2/10\n","19/19 [==============================] - 144s 8s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 3/10\n","19/19 [==============================] - 139s 7s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 4/10\n","19/19 [==============================] - 140s 7s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 5/10\n","19/19 [==============================] - 143s 8s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 6/10\n","19/19 [==============================] - 142s 7s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 7/10\n","19/19 [==============================] - 137s 7s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 8/10\n","19/19 [==============================] - 141s 7s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 9/10\n","19/19 [==============================] - 143s 8s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","Epoch 10/10\n","19/19 [==============================] - 142s 8s/step - loss: 7.6704 - accuracy: 0.5208 - val_loss: 7.6734 - val_accuracy: 0.5550\n","17/17 [==============================] - 12s 686ms/step - loss: 7.6709 - accuracy: 0.5268\n","Test accuracy: 52.68%\n","Loss is : 8\n"]}],"source":["from tensorflow.python import metrics\n","\n","def run_experiment(model):\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=learning_rate, weight_decay=weight_decay\n","    )\n","\n","\n","    model.compile(\n","        optimizer=optimizer,\n","#        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        loss = keras.losses.BinaryCrossentropy(from_logits=False),\n","        metrics=['accuracy']\n","        #     keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","        #     keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        # ],\n","    )\n","  # opt = SGD(learning_rate=1e-6, momentum=0.9)\n","  # model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=opt,metrics=['accuracy'])\n","\n","    # metric = 'val_accuracy'\n","    # ModelCheckpoint(filepath=r\"C:\\Users\\reda.elhail\\Desktop\\checkpoints\\{}\".format(Name), monitor=metric,\n","    #                     verbose=2, save_best_only=True, mode='max')]\n","\n","    checkpoint_filepath = r'/content/drive/MyDrive/Machine Learning project/train/tmp/checkpoint'\n","    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor='accuracy',\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","\n","    history = model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_split=0.1,\n","        callbacks=[checkpoint_callback],\n","    )\n","\n","\n","    model.load_weights(checkpoint_filepath)\n","    loss, accuracy = model.evaluate(x_test, y_test)\n","    \n","    # _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n","    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    # print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","    print(f\"Loss is : {round(loss)}\")\n","\n","    return history\n","\n","\n","vit_classifier = create_vit_classifier()\n","history = run_experiment(vit_classifier)\n"]},{"cell_type":"code","source":["#Start time 22:40\n","model=vit_classifier\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxEUAbMVFmwj","executionInfo":{"status":"ok","timestamp":1670294461958,"user_tz":300,"elapsed":1358,"user":{"displayName":"Yash Garje","userId":"04272032895429331929"}},"outputId":"8d0a8f80-f94c-4e3a-eb89-98d45f97efd5"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_3[0][0]']                \n","                                                                                                  \n"," patches_5 (Patches)            (None, None, 108)    0           ['data_augmentation[0][0]']      \n","                                                                                                  \n"," patch_encoder_2 (PatchEncoder)  (None, 144, 64)     16192       ['patches_5[0][0]']              \n","                                                                                                  \n"," layer_normalization_34 (LayerN  (None, 144, 64)     128         ['patch_encoder_2[0][0]']        \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_16 (Multi  (None, 144, 64)     66368       ['layer_normalization_34[0][0]', \n"," HeadAttention)                                                   'layer_normalization_34[0][0]'] \n","                                                                                                  \n"," add_32 (Add)                   (None, 144, 64)      0           ['multi_head_attention_16[0][0]',\n","                                                                  'patch_encoder_2[0][0]']        \n","                                                                                                  \n"," layer_normalization_35 (LayerN  (None, 144, 64)     128         ['add_32[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_41 (Dense)               (None, 144, 128)     8320        ['layer_normalization_35[0][0]'] \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 144, 128)     0           ['dense_41[0][0]']               \n","                                                                                                  \n"," dense_42 (Dense)               (None, 144, 64)      8256        ['dropout_38[0][0]']             \n","                                                                                                  \n"," dropout_39 (Dropout)           (None, 144, 64)      0           ['dense_42[0][0]']               \n","                                                                                                  \n"," add_33 (Add)                   (None, 144, 64)      0           ['dropout_39[0][0]',             \n","                                                                  'add_32[0][0]']                 \n","                                                                                                  \n"," layer_normalization_36 (LayerN  (None, 144, 64)     128         ['add_33[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_17 (Multi  (None, 144, 64)     66368       ['layer_normalization_36[0][0]', \n"," HeadAttention)                                                   'layer_normalization_36[0][0]'] \n","                                                                                                  \n"," add_34 (Add)                   (None, 144, 64)      0           ['multi_head_attention_17[0][0]',\n","                                                                  'add_33[0][0]']                 \n","                                                                                                  \n"," layer_normalization_37 (LayerN  (None, 144, 64)     128         ['add_34[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_43 (Dense)               (None, 144, 128)     8320        ['layer_normalization_37[0][0]'] \n","                                                                                                  \n"," dropout_40 (Dropout)           (None, 144, 128)     0           ['dense_43[0][0]']               \n","                                                                                                  \n"," dense_44 (Dense)               (None, 144, 64)      8256        ['dropout_40[0][0]']             \n","                                                                                                  \n"," dropout_41 (Dropout)           (None, 144, 64)      0           ['dense_44[0][0]']               \n","                                                                                                  \n"," add_35 (Add)                   (None, 144, 64)      0           ['dropout_41[0][0]',             \n","                                                                  'add_34[0][0]']                 \n","                                                                                                  \n"," layer_normalization_38 (LayerN  (None, 144, 64)     128         ['add_35[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_18 (Multi  (None, 144, 64)     66368       ['layer_normalization_38[0][0]', \n"," HeadAttention)                                                   'layer_normalization_38[0][0]'] \n","                                                                                                  \n"," add_36 (Add)                   (None, 144, 64)      0           ['multi_head_attention_18[0][0]',\n","                                                                  'add_35[0][0]']                 \n","                                                                                                  \n"," layer_normalization_39 (LayerN  (None, 144, 64)     128         ['add_36[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_45 (Dense)               (None, 144, 128)     8320        ['layer_normalization_39[0][0]'] \n","                                                                                                  \n"," dropout_42 (Dropout)           (None, 144, 128)     0           ['dense_45[0][0]']               \n","                                                                                                  \n"," dense_46 (Dense)               (None, 144, 64)      8256        ['dropout_42[0][0]']             \n","                                                                                                  \n"," dropout_43 (Dropout)           (None, 144, 64)      0           ['dense_46[0][0]']               \n","                                                                                                  \n"," add_37 (Add)                   (None, 144, 64)      0           ['dropout_43[0][0]',             \n","                                                                  'add_36[0][0]']                 \n","                                                                                                  \n"," layer_normalization_40 (LayerN  (None, 144, 64)     128         ['add_37[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_19 (Multi  (None, 144, 64)     66368       ['layer_normalization_40[0][0]', \n"," HeadAttention)                                                   'layer_normalization_40[0][0]'] \n","                                                                                                  \n"," add_38 (Add)                   (None, 144, 64)      0           ['multi_head_attention_19[0][0]',\n","                                                                  'add_37[0][0]']                 \n","                                                                                                  \n"," layer_normalization_41 (LayerN  (None, 144, 64)     128         ['add_38[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_47 (Dense)               (None, 144, 128)     8320        ['layer_normalization_41[0][0]'] \n","                                                                                                  \n"," dropout_44 (Dropout)           (None, 144, 128)     0           ['dense_47[0][0]']               \n","                                                                                                  \n"," dense_48 (Dense)               (None, 144, 64)      8256        ['dropout_44[0][0]']             \n","                                                                                                  \n"," dropout_45 (Dropout)           (None, 144, 64)      0           ['dense_48[0][0]']               \n","                                                                                                  \n"," add_39 (Add)                   (None, 144, 64)      0           ['dropout_45[0][0]',             \n","                                                                  'add_38[0][0]']                 \n","                                                                                                  \n"," layer_normalization_42 (LayerN  (None, 144, 64)     128         ['add_39[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_20 (Multi  (None, 144, 64)     66368       ['layer_normalization_42[0][0]', \n"," HeadAttention)                                                   'layer_normalization_42[0][0]'] \n","                                                                                                  \n"," add_40 (Add)                   (None, 144, 64)      0           ['multi_head_attention_20[0][0]',\n","                                                                  'add_39[0][0]']                 \n","                                                                                                  \n"," layer_normalization_43 (LayerN  (None, 144, 64)     128         ['add_40[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_49 (Dense)               (None, 144, 128)     8320        ['layer_normalization_43[0][0]'] \n","                                                                                                  \n"," dropout_46 (Dropout)           (None, 144, 128)     0           ['dense_49[0][0]']               \n","                                                                                                  \n"," dense_50 (Dense)               (None, 144, 64)      8256        ['dropout_46[0][0]']             \n","                                                                                                  \n"," dropout_47 (Dropout)           (None, 144, 64)      0           ['dense_50[0][0]']               \n","                                                                                                  \n"," add_41 (Add)                   (None, 144, 64)      0           ['dropout_47[0][0]',             \n","                                                                  'add_40[0][0]']                 \n","                                                                                                  \n"," layer_normalization_44 (LayerN  (None, 144, 64)     128         ['add_41[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_21 (Multi  (None, 144, 64)     66368       ['layer_normalization_44[0][0]', \n"," HeadAttention)                                                   'layer_normalization_44[0][0]'] \n","                                                                                                  \n"," add_42 (Add)                   (None, 144, 64)      0           ['multi_head_attention_21[0][0]',\n","                                                                  'add_41[0][0]']                 \n","                                                                                                  \n"," layer_normalization_45 (LayerN  (None, 144, 64)     128         ['add_42[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_51 (Dense)               (None, 144, 128)     8320        ['layer_normalization_45[0][0]'] \n","                                                                                                  \n"," dropout_48 (Dropout)           (None, 144, 128)     0           ['dense_51[0][0]']               \n","                                                                                                  \n"," dense_52 (Dense)               (None, 144, 64)      8256        ['dropout_48[0][0]']             \n","                                                                                                  \n"," dropout_49 (Dropout)           (None, 144, 64)      0           ['dense_52[0][0]']               \n","                                                                                                  \n"," add_43 (Add)                   (None, 144, 64)      0           ['dropout_49[0][0]',             \n","                                                                  'add_42[0][0]']                 \n","                                                                                                  \n"," layer_normalization_46 (LayerN  (None, 144, 64)     128         ['add_43[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_22 (Multi  (None, 144, 64)     66368       ['layer_normalization_46[0][0]', \n"," HeadAttention)                                                   'layer_normalization_46[0][0]'] \n","                                                                                                  \n"," add_44 (Add)                   (None, 144, 64)      0           ['multi_head_attention_22[0][0]',\n","                                                                  'add_43[0][0]']                 \n","                                                                                                  \n"," layer_normalization_47 (LayerN  (None, 144, 64)     128         ['add_44[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_53 (Dense)               (None, 144, 128)     8320        ['layer_normalization_47[0][0]'] \n","                                                                                                  \n"," dropout_50 (Dropout)           (None, 144, 128)     0           ['dense_53[0][0]']               \n","                                                                                                  \n"," dense_54 (Dense)               (None, 144, 64)      8256        ['dropout_50[0][0]']             \n","                                                                                                  \n"," dropout_51 (Dropout)           (None, 144, 64)      0           ['dense_54[0][0]']               \n","                                                                                                  \n"," add_45 (Add)                   (None, 144, 64)      0           ['dropout_51[0][0]',             \n","                                                                  'add_44[0][0]']                 \n","                                                                                                  \n"," layer_normalization_48 (LayerN  (None, 144, 64)     128         ['add_45[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," multi_head_attention_23 (Multi  (None, 144, 64)     66368       ['layer_normalization_48[0][0]', \n"," HeadAttention)                                                   'layer_normalization_48[0][0]'] \n","                                                                                                  \n"," add_46 (Add)                   (None, 144, 64)      0           ['multi_head_attention_23[0][0]',\n","                                                                  'add_45[0][0]']                 \n","                                                                                                  \n"," layer_normalization_49 (LayerN  (None, 144, 64)     128         ['add_46[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," dense_55 (Dense)               (None, 144, 128)     8320        ['layer_normalization_49[0][0]'] \n","                                                                                                  \n"," dropout_52 (Dropout)           (None, 144, 128)     0           ['dense_55[0][0]']               \n","                                                                                                  \n"," dense_56 (Dense)               (None, 144, 64)      8256        ['dropout_52[0][0]']             \n","                                                                                                  \n"," dropout_53 (Dropout)           (None, 144, 64)      0           ['dense_56[0][0]']               \n","                                                                                                  \n"," add_47 (Add)                   (None, 144, 64)      0           ['dropout_53[0][0]',             \n","                                                                  'add_46[0][0]']                 \n","                                                                                                  \n"," layer_normalization_50 (LayerN  (None, 144, 64)     128         ['add_47[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," flatten_2 (Flatten)            (None, 9216)         0           ['layer_normalization_50[0][0]'] \n","                                                                                                  \n"," dropout_54 (Dropout)           (None, 9216)         0           ['flatten_2[0][0]']              \n","                                                                                                  \n"," dense_57 (Dense)               (None, 2048)         18876416    ['dropout_54[0][0]']             \n","                                                                                                  \n"," dropout_55 (Dropout)           (None, 2048)         0           ['dense_57[0][0]']               \n","                                                                                                  \n"," dense_58 (Dense)               (None, 1024)         2098176     ['dropout_55[0][0]']             \n","                                                                                                  \n"," dropout_56 (Dropout)           (None, 1024)         0           ['dense_58[0][0]']               \n","                                                                                                  \n"," dense_59 (Dense)               (None, 2)            2050        ['dropout_56[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 21,658,569\n","Trainable params: 21,658,562\n","Non-trainable params: 7\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"MT1oieae19Qk"},"source":["After 100 epochs, the ViT model achieves around 55% accuracy and\n","82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n","as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n","\n","Note that the state of the art results reported in the\n","[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n","the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n","without pre-training, you can try to train the model for more epochs, use a larger number of\n","Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n","Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n","but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n","In practice, it's recommended to fine-tune a ViT model\n","that was pre-trained using a large, high-resolution dataset."]}],"metadata":{"colab":{"provenance":[{"file_id":"1i8s306-bC2-LLeCK0YjhESKzVcEnfORD","timestamp":1670281719816},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/image_classification_with_vision_transformer.ipynb","timestamp":1668727018316}]},"environment":{"name":"tf2-gpu.2-4.m61","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}